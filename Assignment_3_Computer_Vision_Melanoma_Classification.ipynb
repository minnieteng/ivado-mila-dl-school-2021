{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3_Computer Vision_Melanoma Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minnieteng/ivado-mila-dl-school-2021/blob/main/Assignment_3_Computer_Vision_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dayV_PnCxCHy"
      },
      "source": [
        "## Welcome to this Michener Institute & Vector Institute Course!\n",
        "\n",
        "This is a case study in the â€˜AI for Clinician Champions Certificate' program! \n",
        "\n",
        "https://michener.ca/ce_course/ai-for-clinician-champions-certificate-program/ - \n",
        "This program is offered by Michener Institute & Vector Institute for clinicians who wish to learn more about AI in Healthcare. \n",
        "This coding tutorial is optional however, themes from the model will be used in the overall assignment. Learners are enouraged to, at a minimum, look through this tutorial or, run the model for a full learning experience. \n",
        "\n",
        "This week's case study will introduce you to medical image analysis, using melanoma classification as an example. The goal of this case study is to build neural network models to classify whether there is melanoma in skin lesion images. This is a Computer Vision (CV) use-case of AI. CV in AI deals with how models deal with images or video.\n",
        "\n",
        "Instructor: Dr. Devin Singh (@DrDevSK) | Assignment Developer: Jianan Chen | Course Tutors: Jianan Chen, Flora Wan, and S. Alex Yun | Course Director: Shingai Manjengwa (@Tjido) \n",
        " \n",
        "***Never stop learning!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKybW3wFftsC"
      },
      "source": [
        "# Melanoma classification\n",
        "\n",
        "Skin cancer is the most common cancer type. Melanoma, specifically, accounts for 75% of skin cancer deaths, despite being the least common skin cancer. It is estimated by the American Cancer Society that there will be over 100,000 new melanoma cases and more than 7,000 melanoma-related deaths each year. Early detection of melanoma enabled by machine learning may improve patient prognosis. \n",
        "\n",
        "In this case study, we build deep learning models to identify melanomas in images of skin lesions based on data from the SIIM-ISIC Melanoma Classification Challenge. The SSIM-ISIC Melanoma Classfication Challenge is a [Kaggle competition](https://www.kaggle.com/c/siim-isic-melanoma-classification) hosted by [Society for Imaging Informatics in Medicine](https://siim.org/) and [the International Skin Imaging Collaboration](https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main). We will cover several aspects of computer vision with deep learning, including network structures, interpretation of neural networks and dataset constructions.\n",
        "\n",
        "Please read the instructions before running the code. For each block, the instruction will be on the right column.  Please try to avoid refreshing the webpage or restarting the session. If you refresh, please run every block again from the beginning.\n",
        "\n",
        "**Acknowledgements**: The code in this case study is adpated from [Andrada Olteanu](https://www.kaggle.com/andradaolteanu/melanoma-competiton-aug-resnet-effnet-lb-0-91) and [Roman's](https://www.kaggle.com/nroman/melanoma-pytorch-starter-efficientnet/output) work, and the format of the notebook is inspired by the explanatory notebook in [MIDOG challenge](https://imi.thi.de/midog/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzZnsP4SITFU",
        "cellView": "form"
      },
      "source": [
        "#@title Enabling GPU in Google Colab (**Important**)\n",
        "\n",
        "#@markdown - Before we proceed, we need to turn on GPU acceleration for this notebook, this can be done by\n",
        "#@markdown selecting the following menu options: Runtime / Change runtime type / Hardware accelerator.\n",
        "#@markdown Select 'GPU' from the dropdown menu and press 'Save'.\n",
        "#@markdown - Once you have changed the settings, run this block and check the output. If it says \"Device available now: cuda\" then we are good to go!\n",
        "#@markdown - GPU stands for Graphical Processing Unit (compare to CPU, Central Processing Unit). GPUs allow for faster and more efficient processing of images in computing systems.\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device available now:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb1EkKSTb-Ee"
      },
      "source": [
        "#@title Import some python packages { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown - In this case study we focus on images taken by cameras (.jpg, .png, ..) so we import [opencv](https://docs.opencv.org/master/d6/d00/tutorial_py_root.html)\n",
        "#@markdown - For other image modalities e.g. MR, CT and US, we can use specialized packages such as [SimpleITK](https://simpleitk.org/) and [pydicom](https://pydicom.github.io/).\n",
        "#@markdown \n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -U efficientnet_pytorch\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKQ_M3YwcY5f"
      },
      "source": [
        "#@title Download data to Colab { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import zipfile\n",
        "\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1P62dCd6Nu8pkV9YPMb3QCfMBOZUVAT2Y' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1P62dCd6Nu8pkV9YPMb3QCfMBOZUVAT2Y\" -O 'casestudy2.zip' && rm -rf /tmp/cookies.txt\n",
        "\n",
        "with zipfile.ZipFile('casestudy2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('casestudy2')\n",
        "\n",
        "#@markdown The dataset is stored on Google drive as a zip file. When we run the cell\n",
        "#@markdown Colab will retrieve this zip file and unzip it into images and spreadsheets.\n",
        "\n",
        "#@markdown You can have a look at the dataset structure by pressing the folder icon on the left sidebar.\n",
        "\n",
        "SIIM_folder = Path(\"/content/casestudy2/Case 3\") \n",
        "\n",
        "#@markdown Your output should contain **test_clean.csv** and **train_clean.csv**:\n",
        "print(list(SIIM_folder.glob(\"*.*\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9KioTFWj1Sn"
      },
      "source": [
        "#@title Library for visualization and deep learning { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown In this case study we will be using [PyTorch](https://pytorch.org/) for building and training the deep neural network.\n",
        "\n",
        "#@markdown PyTorch is one of the most popular open-source deep learning framework for researchers.\n",
        "import pandas as pd\n",
        "import torchvision.models as models\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold, train_test_split\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import os\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Package imported!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geRSQhH9lOyB"
      },
      "source": [
        "#@title Let us take a look at the files { display-mode: \"form\" }\n",
        "\n",
        "#@markdown The original SIIM-ISIC dataset contains more than 44,000 images. In \n",
        "#@markdown order to speed up training, we randomly selected 600 images \n",
        "#@markdown (300 benign and 300 malignant skin lesions) for our case study and \n",
        "#@markdown split the dataset into 80% for training and 20% for testing.\n",
        "\n",
        "image_folder = SIIM_folder / \"SIIM\"\n",
        "train_csv = SIIM_folder / \"train_clean.csv\"\n",
        "test_csv = SIIM_folder / \"test_clean.csv\"\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "example_df = train_df.loc[:, ['image_name','benign_malignant','target']]\n",
        "\n",
        "print(f'There are {len(train_df)} images for training and {len(test_df)} images for testing.')\n",
        "print('First five rows of our dataframe:')\n",
        "print(example_df.head())\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "columns = 5\n",
        "rows = 1\n",
        "ax = []\n",
        "\n",
        "for i, img in enumerate(train_df.image_name[0:5]):\n",
        "    img_path = os.path.join('/content/casestudy2/Case 3/SIIM/', img + '.jpg')\n",
        "    imgs = cv2.imread(img_path)\n",
        "    imgs = cv2.cvtColor(imgs, cv2.COLOR_BGR2RGB)\n",
        "    imgs = np.array(imgs)\n",
        "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
        "    ax[-1].set_title(img)  # set title\n",
        "    plt.imshow(imgs)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BMhHX65nvgK"
      },
      "source": [
        "#@title Set seed to ensure the results are reproducible { display-mode: \"form\" }\n",
        "\n",
        "#@markdown It's important to make our results reproducible so that it's easier to debug your model and fairly evaluate model performance.\n",
        "#@markdown This can be achived by setting a random seed for all the packages we have using.\n",
        "def set_seed(seed=1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "print('The defualt seed for this notebook is 1234.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R48DoO71pB7C"
      },
      "source": [
        "# Neural Network\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1fCSKpKfmW-mrAdAW-rL40lixy3thHaGw)\n",
        "**Image credit**: [Google AI blog](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)\n",
        "\n",
        "Various neural network architectures are proposed in the recent years. The above\n",
        "figure shows the trade-off between the number of parameters (demand of computing power & training time) and performance of a few popular network structures. In this study we will implement EfficientNet-B2 and ResNet-50 for comparision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMXzZnDcqjmL"
      },
      "source": [
        "#@title Create dataset and dataloader { display-mode: \"form\" }\n",
        "\n",
        "#@markdown - To train neural networks on your own data, you need to first create\n",
        "#@markdown custom datasets and dataloaders for your data.\n",
        "#@markdown - We create a SIIM melanoma dataset and apply augmentations to our data\n",
        "#@markdown to enhance our dataset.\n",
        "#@markdown - Data augmentation is a technique that is used in almost all computer vision projects, \n",
        "#@markdown for generating more data and taking into account the variations in unseen test data.\n",
        "#@markdown - You will have a chance to tune the data augmentations parameters in the following cells.\n",
        "#@markdown - A review on data augmentation if you are interested: [link to paper](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import *\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class MelanomaDataset(Dataset):\n",
        "    def __init__(self, dataframe, vertical_flip, horizontal_flip, color_jitter=0.3,\n",
        "                 is_train=True, is_valid=False, is_test=False, is_original=False):\n",
        "        self.dataframe, self.is_train, self.is_valid, self.is_original = dataframe, is_train, is_valid, is_original\n",
        "        self.vertical_flip, self.horizontal_flip = vertical_flip, horizontal_flip\n",
        "        self.color_jitter=color_jitter\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "        # Data Augmentation (custom for each dataset type)\n",
        "        if is_train or is_test:\n",
        "            self.transform = Compose([ToTensor(),\n",
        "                                      RandomResizedCrop((224,224), (0.6, 1.0)),\n",
        "                                      RandomAffine(90, scale=(0.8, 1.2)),\n",
        "                                      RandomHorizontalFlip(p=self.horizontal_flip),\n",
        "                                      RandomVerticalFlip(p=self.vertical_flip),\n",
        "                                      ColorJitter(brightness=self.color_jitter, contrast=self.color_jitter, saturation=self.color_jitter, hue=0.1),\n",
        "                                      normalize])\n",
        "        elif is_valid:\n",
        "            self.transform = Compose([ToTensor(),\n",
        "                                      Resize((224,224)),\n",
        "                                      normalize])\n",
        "        else:\n",
        "            self.transform = Compose([ToTensor(),\n",
        "                                      Resize((224,224))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Select path and read image\n",
        "        image_name = self.dataframe['image_name'][index]\n",
        "        image_path = os.path.join('/content/casestudy2/Case 3/SIIM/', image_name + '.jpg') \n",
        "        image = cv2.imread(image_path)\n",
        "        # For this image also import .csv information (sex, age, anatomy)\n",
        "        csv_data = np.array(self.dataframe.iloc[index][['sex', 'age', 'anatomy']].values,\n",
        "                            dtype=np.float32)\n",
        "\n",
        "        # Apply transforms\n",
        "        image = self.transform(image)\n",
        "        # Extract image from dictionary\n",
        "\n",
        "        # If train/valid: image + class | If test: only image\n",
        "        if self.is_train or self.is_valid or self.is_original:\n",
        "            return (image, csv_data), self.dataframe['target'][index]\n",
        "        else:\n",
        "            return (image, csv_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkEaEEW8sXe-",
        "cellView": "form"
      },
      "source": [
        "#@title Define network structures\n",
        "#@markdown In this block we define the network structures.\n",
        "from torch import nn\n",
        "import torch\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torchvision.models import resnet50\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EfficientNetwork(nn.Module):\n",
        "    def __init__(self, output_size, b4=False, b2=False):\n",
        "        super().__init__()\n",
        "        self.b4, self.b2 = b4, b2\n",
        "\n",
        "        # Define Feature part (IMAGE)\n",
        "        if b4:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "        elif b2:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        else:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "\n",
        "        # Define Classification part\n",
        "        if b4:\n",
        "            self.classification = nn.Sequential(nn.Linear(1792, output_size))\n",
        "        elif b2:\n",
        "            self.classification = nn.Sequential(nn.Linear(1408, output_size))\n",
        "        else:\n",
        "            self.classification = nn.Sequential(nn.Linear(2560, output_size))\n",
        "\n",
        "    def forward(self, image, prints=False):\n",
        "\n",
        "        # IMAGE CNN\n",
        "        image = self.features.extract_features(image)\n",
        "\n",
        "        if self.b4:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1792)\n",
        "        elif self.b2:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1408)\n",
        "        else:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 2560)\n",
        "\n",
        "        # CLASSIF\n",
        "        out = self.classification(image)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet50Network(nn.Module):\n",
        "    def __init__(self, output_size, no_columns):\n",
        "        super().__init__()\n",
        "        self.no_columns, self.output_size = no_columns, output_size\n",
        "\n",
        "        # Define Feature part (IMAGE)\n",
        "        self.features = resnet50(pretrained=True)  # 1000 neurons out\n",
        "\n",
        "        # Define Classification part\n",
        "        self.classification = nn.Linear(1000, output_size)\n",
        "\n",
        "    def forward(self, image, prints=False):\n",
        "\n",
        "        # Image CNN\n",
        "        image = self.features(image)\n",
        "\n",
        "        # CLASSIF\n",
        "        out = self.classification(image)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TwBkNLQyia6",
        "cellView": "form"
      },
      "source": [
        "#@title Define training function\n",
        "#@markdown In this block we implemete a function for training, validating and testing the network.\n",
        "def train_folds(preds_submission, model, train_df, test_df, flip=0.5, color_jitter=0.3, version='v1'):\n",
        "    # Creates a .txt file that will contain the logs\n",
        "    f = open(f\"logs_{version}.txt\", \"w+\")\n",
        "\n",
        "    # ----- STATICS -----\n",
        "    indices = np.arange(len(train_df))\n",
        "    train_index, valid_index = train_test_split(indices, test_size=0.2, random_state=1234)\n",
        "    train_data = train_df.iloc[train_index].reset_index(drop=True)\n",
        "    valid_data = train_df.iloc[valid_index].reset_index(drop=True)\n",
        "\n",
        "    # Append to .txt\n",
        "    with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "        print('-' * 10, 'Training', '-' * 10, file=f)\n",
        "    print('-' * 10, 'Training', '-' * 10)\n",
        "\n",
        "    # --- Create Instances ---\n",
        "    # Best ROC score in this fold\n",
        "    best_roc = None\n",
        "    # Reset patience before every fold\n",
        "    patience_f = patience\n",
        "\n",
        "    # Initiate the model\n",
        "    model = model\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max',\n",
        "                                  patience=lr_patience, verbose=True, factor=lr_factor)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Create Data instances\n",
        "    train = MelanomaDataset(train_data, vertical_flip=flip, horizontal_flip=flip, color_jitter=color_jitter,\n",
        "                            is_train=True, is_valid=False, is_test=False)\n",
        "    valid = MelanomaDataset(valid_data, vertical_flip=flip, horizontal_flip=flip, color_jitter=color_jitter,\n",
        "                            is_train=False, is_valid=True, is_test=False)\n",
        "    # Read in test data | Remember! We're using data augmentation like we use for Train data.\n",
        "    test = MelanomaDataset(test_df, vertical_flip=flip, horizontal_flip=flip, color_jitter=color_jitter,\n",
        "                           is_train=False, is_valid=False, is_test=True)\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader = DataLoader(train, batch_size=batch_size1, shuffle=True, num_workers=num_workers)\n",
        "    # shuffle=False! Otherwise function won't work!!!\n",
        "    # how do I know? ^^\n",
        "    valid_loader = DataLoader(valid, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # === EPOCHS ===\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        correct = 0\n",
        "        train_losses = 0\n",
        "\n",
        "        # === TRAIN ===\n",
        "        # Sets the module in training mode.\n",
        "        model.train()\n",
        "\n",
        "        for (images, csv_data), labels in train_loader:\n",
        "            # Save them to device\n",
        "            images = images.type(torch.float32).cuda()\n",
        "            csv_data = csv_data.type(torch.float32).cuda()\n",
        "            labels = labels.type(torch.float32).cuda()\n",
        "\n",
        "            # Clear gradients first; very important, usually done BEFORE prediction\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Log Probabilities & Backpropagation\n",
        "            out = model(images, csv_data)\n",
        "            loss = criterion(out, labels.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --- Save information after this batch ---\n",
        "            # Save loss\n",
        "            train_losses += loss.item()\n",
        "            # From log probabilities to actual probabilities\n",
        "            train_preds = torch.round(torch.sigmoid(out))  # 0 and 1\n",
        "            # Number of correct predictions\n",
        "            correct += (train_preds.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
        "\n",
        "        # Compute Train Accuracy\n",
        "        train_acc = correct / len(train_index)\n",
        "\n",
        "        # === EVAL ===\n",
        "        # Sets the model in evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Create matrix to store evaluation predictions (for accuracy)\n",
        "        valid_preds = torch.zeros(size=(len(valid_index), 1), device=device, dtype=torch.float32)\n",
        "\n",
        "        # Disables gradients (we need to be sure no optimization happens)\n",
        "        with torch.no_grad():\n",
        "            for k, ((images, csv_data), labels) in enumerate(valid_loader):\n",
        "                images = images.type(torch.float32).cuda()\n",
        "                csv_data = csv_data.type(torch.float32).cuda()\n",
        "                labels = labels.type(torch.float32).cuda()\n",
        "                # images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                # csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "                # labels = torch.tensor(labels, device=device, dtype=torch.float32)\n",
        "\n",
        "                out = model(images, csv_data)\n",
        "                pred = torch.sigmoid(out)\n",
        "                valid_preds[k * images.shape[0]: k * images.shape[0] + images.shape[0]] = pred\n",
        "\n",
        "            # Compute accuracy\n",
        "            valid_acc = accuracy_score(valid_data['target'].values,\n",
        "                                       torch.round(valid_preds.cpu()))\n",
        "            # Compute ROC\n",
        "            valid_roc = roc_auc_score(valid_data['target'].values,\n",
        "                                      valid_preds.cpu())\n",
        "\n",
        "            # Compute time on Train + Eval\n",
        "            duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
        "\n",
        "            # PRINT INFO\n",
        "            # Append to .txt file\n",
        "            with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "                print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'. \\\n",
        "                      format(duration, epoch + 1, epochs, train_losses, train_acc, valid_acc, valid_roc), file=f)\n",
        "            # Print to console\n",
        "            print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'. \\\n",
        "                  format(duration, epoch + 1, epochs, train_losses, train_acc, valid_acc, valid_roc))\n",
        "\n",
        "            # === SAVE MODEL ===\n",
        "\n",
        "            # Update scheduler (for learning_rate)\n",
        "            scheduler.step(valid_roc)\n",
        "\n",
        "            # # Update best_roc\n",
        "            # if not best_roc:  # If best_roc = None\n",
        "            #     best_roc = valid_roc\n",
        "            #     torch.save(model.state_dict(),\n",
        "            #                f\"Epoch{epoch + 1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
        "            #     continue\n",
        "\n",
        "            # if valid_roc > best_roc:\n",
        "            #     best_roc = valid_roc\n",
        "            #     # Reset patience (because we have improvement)\n",
        "            #     patience_f = patience\n",
        "            #     # torch.save(model.state_dict(),\n",
        "            #     #            f\"Epoch{epoch + 1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
        "            # else:\n",
        "\n",
        "            #     # Decrease patience (no improvement in ROC)\n",
        "            #     patience_f = patience_f - 1\n",
        "            #     if patience_f == 0:\n",
        "            #         with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "            #             print('Early stopping (no improvement since 3 models) | Best ROC: {}'. \\\n",
        "            #                   format(best_roc), file=f)\n",
        "            #         print('Early stopping (no improvement since 3 models) | Best ROC: {}'. \\\n",
        "            #               format(best_roc))\n",
        "            #         break\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGUyQrSQz-Em",
        "cellView": "form"
      },
      "source": [
        "#@title Define hyperparameters\n",
        "#@markdown In this block we define the hyperparameters used in training, for example number of epochs, learning rate and batch size. \n",
        "#@markdown Choice of hyperparameters can have a big impact on network performance. \n",
        "\n",
        "set_seed()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# ----- STATICS, don't change -----\n",
        "output_size = 1\n",
        "csv_columns = ['sex', 'age', 'anatomy']\n",
        "no_columns = 3\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "train_len = len(train_df)\n",
        "test_len = len(test_df)\n",
        "# -------------------\n",
        "\n",
        "# Out of Fold Predictions\n",
        "oof = np.zeros(shape=(train_len, 1))\n",
        "\n",
        "# Predictions\n",
        "preds_submission = torch.zeros(size=(test_len, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "# ----- Hyperparameters -----\n",
        "# probability of applying vertical/horinzontal flip\n",
        "flip = 0.5\n",
        "color_jitter = 0.3\n",
        "# number of epochs for training\n",
        "epochs = 10\n",
        "# learning rate\n",
        "learning_rate = 0.0005\n",
        "# batch_size\n",
        "batch_size1 = 4\n",
        "batch_size2 = 4\n",
        "# ----- -----\n",
        "patience = 3\n",
        "num_workers = 0\n",
        "weight_decay = 0.0\n",
        "lr_patience = 1  # 1 model not improving until lr is decreasing\n",
        "lr_factor = 0.4  # by how much the lr is decreasing\n",
        "# -------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2lcCbf0BwBg",
        "cellView": "form"
      },
      "source": [
        "#@title EfficientNet-B2\n",
        "#@markdown - EfficientNet is a state-of-the-art neural network model. It has both superior performance and lower number of parameters compared with previous models.\n",
        "#@markdown - The number in the model name reflects the depths/parameters of the model, where a larger number means a larger model. Here we use B2 out of B0-B7.\n",
        "#@markdown - This training is going to take a while. Depending on the GPU you are getting with colab, it may range from **25 to 50 minutes**.\n",
        "#@markdown - For the EfficientNet B2 model, you may reach a validation AUC of around 0.85\n",
        "\n",
        "#@markdown You can tune some hyperparameters using the slide bars below.\n",
        "\n",
        "#@markdown - **Flip probability** controls the probability for applying horizontal and vertical flipping to the images, where 0 means not flipping.\n",
        "#@markdown - **Color jitter** controls the strengths of color alterations applied to the images, including brightness, contrast, saturation and hue, where 0 means no color augmentations. \n",
        "#@markdown - The default value may not be the optimal setting. In general we will get \n",
        "#@markdown better performance with higher values, until the performance drops when color jitter value becomes too large.\n",
        "flip = 0.5 #@param {type:\"slider\", min:0, max:0.5, step:0.1}\n",
        "color_jitter = 0.3 #@param {type:\"slider\", min:0.0, max:1, step:0.1}\n",
        "#@markdown **Note:** after you change the hyperparameters you will need to rerun this cell to see the difference in performance. This will take another 25-50 minutes and is optional.\n",
        "#epochs = 10 #@param {type:\"slider\", min:3, max:20, step:1}\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "# --- EffNet B2 ---\n",
        "model = EfficientNetwork(output_size=output_size, b4=False, b2=True).to(device)\n",
        "\n",
        "# # ===== Uncomment and Train =====\n",
        "trained_model = train_folds(preds_submission = preds_submission, model = model, train_df = train_df, test_df=test_df, flip = flip, color_jitter=color_jitter, version = 'v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnzBw7UcBX5f",
        "cellView": "form"
      },
      "source": [
        "#@title ResNet-50 (**optional**)\n",
        "#@markdown - ResNet is one of the most widely used and successful neural network structures [link to paper](https://arxiv.org/abs/1512.03385).\n",
        "#@markdown - If you are interested, run this block and compare the performance of ResNet-50 with EfficientNet-B2.\n",
        "#@markdown - This is likely going to take longer to train compared to EfficientNet-B2. - This training is going to take a while. Depending on the GPU you are getting with colab, it may range from **45 to 75 minutes**.\n",
        "#@markdown - For the ResNet50 model, you may reach an validation AUC of around 0.80\n",
        "\n",
        "#@markdown You can change the augmentation parameters using the slide bars below.\n",
        "\n",
        "#@markdown - **Flip probability** controls the probability for applying horizontal and vertical flipping to the images.\n",
        "#@markdown - **Color jitter** controls the strengths of color alterations applied to the images, including brightness, contrast, saturation and hue. \n",
        "#@markdown - The default value may not be the optimal settings. In general we will get \n",
        "#@markdown better performance with higher values, until the performance drops when color jitter value becomes too large.\n",
        "flip = 0.5 #@param {type:\"slider\", min:0, max:0.5, step:0.1}\n",
        "color_jitter = 0.3 #@param {type:\"slider\", min:0.0, max:1, step:0.1}\n",
        "#@markdown **Note:** after you change the hyperparameters you will need to rerun this cell to see the difference in performance. This will take another 45-75 minutes and is optional.\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "#--- ResNet50 ---\n",
        "model = ResNet50Network(output_size=output_size, no_columns=no_columns).to(device)\n",
        "trained_model = train_folds(preds_submission = preds_submission, model = model, train_df = train_df, test_df=test_df, flip = flip, color_jitter=color_jitter, version = 'v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NPx7yxEEA9T"
      },
      "source": [
        "# Interpreting neural networks\n",
        "\n",
        "Machine learning, especially deep learning algorithms are sometimes referred to as a black box, as we just feed in some data and receive an output, and we usually don't know what happens in the middle. Interpretation of neural networks has been an active field of research and there are a few interesting and powerful techniques that can open the 'black box' for us. For example, [GradCAM](https://arxiv.org/abs/1610.02391) and its variations can highlight the regions that affects the decision of neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrckJ34lySfd",
        "cellView": "form"
      },
      "source": [
        "#@title Define GradCAM, GradCAM++ and utility functions\n",
        "class GradCAM(object):\n",
        "    \"\"\"Calculate GradCAM salinecy map.\n",
        "\n",
        "    A simple example:\n",
        "\n",
        "        # initialize a model, model_dict and gradcam\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        model_dict = dict(model_type='resnet', arch=resnet, layer_name='layer4', input_size=(224, 224))\n",
        "        gradcam = GradCAM(model_dict)\n",
        "\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcam(normed_img, class_idx=10)\n",
        "\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model_dict (dict): a dictionary that contains 'model_type', 'arch', layer_name', 'input_size'(optional) as keys.\n",
        "        verbose (bool): whether to print output size of the saliency map givien 'layer_name' and 'input_size' in model_dict.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict, verbose=False):\n",
        "        model_type = model_dict['type']\n",
        "        layer_name = model_dict['layer_name']\n",
        "        self.model_arch = model_dict['arch']\n",
        "\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        if 'vgg' in model_type.lower():\n",
        "            target_layer = find_vgg_layer(self.model_arch, layer_name)\n",
        "        elif 'resnet' in model_type.lower():\n",
        "            target_layer = find_resnet_layer(self.model_arch, layer_name)\n",
        "        elif 'densenet' in model_type.lower():\n",
        "            target_layer = find_densenet_layer(self.model_arch, layer_name)\n",
        "        elif 'alexnet' in model_type.lower():\n",
        "            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n",
        "        elif 'squeezenet' in model_type.lower():\n",
        "            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n",
        "        elif 'efficientnet' in model_type.lower():\n",
        "            target_layer = self.model_arch.features._conv_head\n",
        "\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        if verbose:\n",
        "            try:\n",
        "                input_size = model_dict['input_size']\n",
        "            except KeyError:\n",
        "                print(\"please specify size of input image in model_dict. e.g. {'input_size':(224, 224)}\")\n",
        "                pass\n",
        "            else:\n",
        "                device = 'cuda' if next(self.model_arch.parameters()).is_cuda else 'cpu'\n",
        "                self.model_arch(torch.zeros(1, 3, *(input_size), device=device))\n",
        "                print('saliency_map size :', self.activations['value'].shape[2:])\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: input image with shape of (1, 3, H, W)\n",
        "            class_idx (int): class index for calculating GradCAM.\n",
        "                    If not specified, the class index that makes the highest model prediction score will be used.\n",
        "        Return:\n",
        "            mask: saliency map of the same spatial dimension with input\n",
        "            logit: model output\n",
        "        \"\"\"\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        # alpha = F.relu(gradients.view(b, k, -1)).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit\n",
        "\n",
        "    def __call__(self, input, class_idx=None, retain_graph=False):\n",
        "        return self.forward(input, class_idx, retain_graph)\n",
        "\n",
        "\n",
        "class GradCAMpp(GradCAM):\n",
        "    \"\"\"Calculate GradCAM++ salinecy map.\n",
        "\n",
        "    A simple example:\n",
        "\n",
        "        # initialize a model, model_dict and gradcampp\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)\n",
        "        resnet.eval()\n",
        "        model_dict = dict(model_type='resnet', arch=resnet, layer_name='layer4', input_size=(224, 224))\n",
        "        gradcampp = GradCAMpp(model_dict)\n",
        "\n",
        "        # get an image and normalize with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "        img = load_img()\n",
        "        normed_img = normalizer(img)\n",
        "\n",
        "        # get a GradCAM saliency map on the class index 10.\n",
        "        mask, logit = gradcampp(normed_img, class_idx=10)\n",
        "\n",
        "        # make heatmap from mask and synthesize saliency map using heatmap and img\n",
        "        heatmap, cam_result = visualize_cam(mask, img)\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model_dict (dict): a dictionary that contains 'model_type', 'arch', layer_name', 'input_size'(optional) as keys.\n",
        "        verbose (bool): whether to print output size of the saliency map givien 'layer_name' and 'input_size' in model_dict.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dict, verbose=False):\n",
        "        super(GradCAMpp, self).__init__(model_dict, verbose)\n",
        "\n",
        "    def forward(self, input, class_idx=None, retain_graph=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: input image with shape of (1, 3, H, W)\n",
        "            class_idx (int): class index for calculating GradCAM.\n",
        "                    If not specified, the class index that makes the highest model prediction score will be used.\n",
        "        Return:\n",
        "            mask: saliency map of the same spatial dimension with input\n",
        "            logit: model output\n",
        "        \"\"\"\n",
        "        b, c, h, w = input.size()\n",
        "\n",
        "        logit = self.model_arch(input)\n",
        "        if class_idx is None:\n",
        "            score = logit[:, logit.max(1)[-1]].squeeze()\n",
        "        else:\n",
        "            score = logit[:, class_idx].squeeze()\n",
        "\n",
        "        self.model_arch.zero_grad()\n",
        "        score.backward(retain_graph=retain_graph)\n",
        "        gradients = self.gradients['value']  # dS/dA\n",
        "        activations = self.activations['value']  # A\n",
        "        b, k, u, v = gradients.size()\n",
        "\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "                      activations.mul(gradients.pow(3)).view(b, k, u * v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        positive_gradients = F.relu(score.exp() * gradients)  # ReLU(dY/dA) == ReLU(exp(S)*dS/dA))\n",
        "        weights = (alpha * positive_gradients).view(b, k, u * v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "\n",
        "        return saliency_map, logit\n",
        "\n",
        "def visualize_cam(mask, img):\n",
        "    \"\"\"Make heatmap from mask and synthesize GradCAM result image using heatmap and img.\n",
        "    Args:\n",
        "        mask (torch.tensor): mask shape of (1, 1, H, W) and each element has value in range [0, 1]\n",
        "        img (torch.tensor): img shape of (1, 3, H, W) and each pixel value is in range [0, 1]\n",
        "\n",
        "    Return:\n",
        "        heatmap (torch.tensor): heatmap img shape of (3, H, W)\n",
        "        result (torch.tensor): synthesized GradCAM result of same shape with heatmap.\n",
        "    \"\"\"\n",
        "    mask, img = mask.cpu(), img.cpu()\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask.squeeze()), cv2.COLORMAP_JET)\n",
        "    heatmap = torch.from_numpy(heatmap).permute(2, 0, 1).float().div(255)\n",
        "    b, g, r = heatmap.split(1)\n",
        "    heatmap = torch.cat([r, g, b])\n",
        "\n",
        "    result = heatmap + img.cpu()\n",
        "    result = result.div(result.max()).squeeze()\n",
        "\n",
        "    return heatmap, result\n",
        "\n",
        "\n",
        "def find_resnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'conv1'\n",
        "            target_layer_name = 'layer1'\n",
        "            target_layer_name = 'layer1_basicblock0'\n",
        "            target_layer_name = 'layer1_basicblock0_relu'\n",
        "            target_layer_name = 'layer1_bottleneck0'\n",
        "            target_layer_name = 'layer1_bottleneck0_conv1'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample'\n",
        "            target_layer_name = 'layer1_bottleneck0_downsample_0'\n",
        "            target_layer_name = 'avgpool'\n",
        "            target_layer_name = 'fc'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    if 'layer' in target_layer_name:\n",
        "        hierarchy = target_layer_name.split('_')\n",
        "        layer_num = int(hierarchy[0].lstrip('layer'))\n",
        "        if layer_num == 1:\n",
        "            target_layer = arch.layer1\n",
        "        elif layer_num == 2:\n",
        "            target_layer = arch.layer2\n",
        "        elif layer_num == 3:\n",
        "            target_layer = arch.layer3\n",
        "        elif layer_num == 4:\n",
        "            target_layer = arch.layer4\n",
        "        else:\n",
        "            raise ValueError('unknown layer : {}'.format(target_layer_name))\n",
        "\n",
        "        if len(hierarchy) >= 2:\n",
        "            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n",
        "            target_layer = target_layer[bottleneck_num]\n",
        "\n",
        "        if len(hierarchy) >= 3:\n",
        "            target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "        if len(hierarchy) == 4:\n",
        "            target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    else:\n",
        "        target_layer = arch._modules[target_layer_name]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_densenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_transition1'\n",
        "            target_layer_name = 'features_transition1_norm'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n",
        "            target_layer_name = 'classifier'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) >= 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    if len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_vgg_layer(arch, target_layer_name):\n",
        "    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_42'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_alexnet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features'\n",
        "            target_layer_name = 'features_0'\n",
        "            target_layer_name = 'classifier'\n",
        "            target_layer_name = 'classifier_0'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "\n",
        "    if len(hierarchy) >= 1:\n",
        "        target_layer = arch.features\n",
        "\n",
        "    if len(hierarchy) == 2:\n",
        "        target_layer = target_layer[int(hierarchy[1])]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def find_squeezenet_layer(arch, target_layer_name):\n",
        "    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "    Args:\n",
        "        arch: default torchvision densenet models\n",
        "        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n",
        "            target_layer_name = 'features_12'\n",
        "            target_layer_name = 'features_12_expand3x3'\n",
        "            target_layer_name = 'features_12_expand3x3_activation'\n",
        "\n",
        "    Return:\n",
        "        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n",
        "    \"\"\"\n",
        "    hierarchy = target_layer_name.split('_')\n",
        "    target_layer = arch._modules[hierarchy[0]]\n",
        "\n",
        "    if len(hierarchy) >= 2:\n",
        "        target_layer = target_layer._modules[hierarchy[1]]\n",
        "\n",
        "    if len(hierarchy) == 3:\n",
        "        target_layer = target_layer._modules[hierarchy[2]]\n",
        "\n",
        "    elif len(hierarchy) == 4:\n",
        "        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n",
        "\n",
        "    return target_layer\n",
        "\n",
        "\n",
        "def denormalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.mul(std).add(mean)\n",
        "\n",
        "\n",
        "def normalize(tensor, mean, std):\n",
        "    if not tensor.ndimension() == 4:\n",
        "        raise TypeError('tensor should be 4D')\n",
        "\n",
        "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
        "\n",
        "    return tensor.sub(mean).div(std)\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return self.do(tensor)\n",
        "\n",
        "    def do(self, tensor):\n",
        "        return normalize(tensor, self.mean, self.std)\n",
        "\n",
        "    def undo(self, tensor):\n",
        "        return denormalize(tensor, self.mean, self.std)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM6MRyhx43Kc"
      },
      "source": [
        "#@title Visualizing network activations { display-mode: \"form\" }\n",
        "\n",
        "#@markdown - Interpretability is a very important factor in deep learning, especially healthcare with deep learning. \n",
        "#@markdown Interpretable deep learning models are more reliable and can sometimes result in new insights in the disease. \n",
        "#@markdown - Here we use GradCAM++ to visualize the activations, i.e. which\n",
        "#@markdown part of the image leads to the predictions and showed the performance in 4 randomly selected images.\n",
        "#@markdown - As shown in the images, the model identified the region of interest correctly for the first, second and fourth image but \n",
        "#@markdown was confused by the artifacts in the third image.\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "model = EfficientNetwork(output_size=1, b4=False, b2=True).to(device)\n",
        "# model.load_state_dict(torch.load('working//Epoch15_ValidAcc_0.774_ROC_0.852.pth'))\n",
        "model.eval()\n",
        "\n",
        "train_df = pd.read_csv(train_csv)\n",
        "train_df = train_df.iloc[40:45, :].reset_index(drop=True)\n",
        "\n",
        "model_dict = dict(type='efficientnet', arch=model, layer_name='swish', input_size=(224, 224))\n",
        "model_gradcampp = GradCAMpp(model_dict, True)\n",
        "\n",
        "# Data object and Loader\n",
        "example_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5,\n",
        "                                is_train=False, is_valid=True, is_test=False)\n",
        "example_loader = torch.utils.data.DataLoader(example_data, batch_size=4, shuffle=False)\n",
        "\n",
        "original_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5,\n",
        "                                is_train=False, is_valid=False, is_test=False, is_original=True)\n",
        "original_loader = torch.utils.data.DataLoader(original_data, batch_size=4, shuffle=False)\n",
        "\n",
        "for (image, csv_data), labels in example_loader:\n",
        "    image_example = image\n",
        "    break\n",
        "\n",
        "for (image, csv_data), labels in original_loader:\n",
        "    original_example = image\n",
        "    break\n",
        "\n",
        "images = []\n",
        "\n",
        "for i in range(len(image_example)):\n",
        "    current_image = image_example[i].view(1,3,224,224)\n",
        "    current_original = original_example[i].view(1,3,224,224)\n",
        "\n",
        "    mask, _ = model_gradcampp(current_image.cuda())\n",
        "    heatmap, result = visualize_cam(mask, current_original)\n",
        "    images.append(torch.stack([current_original.squeeze().cpu(), heatmap, result], 0))\n",
        "\n",
        "print(len(images))\n",
        "images = make_grid(torch.cat(images, 0), nrow=3)\n",
        "output_dir = 'outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_name = 'cam.png'\n",
        "output_path = os.path.join(output_dir, output_name)\n",
        "save_image(images, output_path)\n",
        "PIL.Image.open(output_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xowQim0NBEiN"
      },
      "source": [
        "# Discussion on the performance of the model\n",
        "\n",
        "- The neural network model we trained achieved ~0.85 AUC and which is not too bad. If we use the whole SIIM-ISIC data to train our model we can reach ~0.95 AUC.\n",
        "- However, no matter how good the model performs on this dataset, the model is not ready to be applied in clinic yet.\n",
        "- For example, most images in this dataset are from Caucasian patients. Skin cancer in people of color may have very different clinical presentations [(Paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757062/). If we use our current model to screen patients of color, the model is very likely to misdiagnose some cases. \n",
        "- One way to solve this problem is to add skin cancer images from people of color to our training dataset. It's important to think through the use cases and potential biases before collecting and constructing your custom datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUwdByLJxIK5"
      },
      "source": [
        "Congratulations, you have completed case study 3 in the â€˜AI for Clinician Champions Certificate' program!  \n",
        "https://michener.ca/ce_course/ai-for-clinician-champions-certificate-program/\n",
        "\n",
        "This program is offered by Michener Institute & Vector Institute for clinicians who wish to learn more about AI in Healthcare.\n",
        "\n",
        "Instructor: Dr. Devin Singh (@DrDevSK) | Assignment Developer: Jianan Chen | Course Tutors: Jianan Chen, Flora Wan, and S. Alex Yun | Course Director: Shingai Manjengwa (@Tjido) \n",
        "\n",
        "***Never stop learning!***"
      ]
    }
  ]
}